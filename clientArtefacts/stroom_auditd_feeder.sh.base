#!/bin/bash

# Release 2.5  - 20231206 Burn Alting - burn.alting@gmail.com
#  - Minor formatting changes
#  - Set a version string for the agent if not an rpm deployment
# Release 2.4  - 20231028 Burn Alting - burn.alting@gmail.com
#  - Protect clean_store() from infinite loop and passed directory missing
#  - Change health values to prefix human readable form of the directory being checked
# Release 2.3  - 20230903 Burn Alting - burn.alting@gmail.com
#   - If _AUDIT_ROOT_ is not set, then use a default of /opt/stroom/auditd
# Release 2.2  - 20230807 Burn Alting - burn@swtf.dyndns.org
#   - Allow one to source a configuration file that can over-ride environment variables
#   - Perform find -delete rather than -exec rm, use -execdir rather than -exec
#   - Use ss rather than netstat (a little faster)
#   - Switch from backticks to $( ) notation for command substitution for clearer code
#   - Where possible, use a fix-strings grep (-F)
#   - Support Ubuntu when forming release strings (VERSION header variable)
# Release 2.1  0 20230613 Burn Alting - burn@swtf.dyndns.org
#   - Change execution of egrep(1) command to grep -E
# Release 2.0  - 20230220 Burn Alting - burn@swtf.dyndns.org
#   - Generate a 'disconnect package file' which can be manually moved to a network where
#     the collected audit can be posted to a stroom proxy.
#   - Emit a message when the random sleep has ended
#   - If a post fails, check for a corrupt gzip and delete
# Release 1.10 - 20221219 Burn Alting - burn@swtf.dyndns.org
#   - Make install location generic (_AUDIT_ROOT_)
#   - Print URL on error/success log
#   - Don't exit on failure to process an audit file
#   - Fixed bug where curl error code not printed
# Release 1.9  - 20210825 Burn Alting - burn@swtf.dyndns.org
#   - Stop attempting to post files after three failed attempts (times the number of URL destinations)
#     So with a single URL and a C_TMO of 37, then it will take approx 3 x 37 seconds before exit
#     With say two URLs and a resultant C_TMO of 10, then it will take approx 3 x (2 x 10) seconds before exit
#   - Allow user to specific retention values (FAILED_RETENTION, FAILED_MAX) on the command line
# Release 1.8  - 20210602 Burn Alting - burn@swtf.dyndns.org
#   - Ensure setting LC_TIME is not over-ridden by a LC_ALL="somevalue" when executing ausearch
#   - Gain some disk storage health items such as df of /var/log/audit, /usr/securtity/auditd
# Release 1.7  - 20201229 Burn Alting - burn@swtf.dyndns.org
#   - Modify gain_iana_timezone to sed out not just zoneinfo but zoneinfo/(posix|right|leaps)
# Release 1.6   Burn Alting 20200619
#   - Automatically lower C_TMO if multiple urls
# Release 1.5   Burn Alting 20200413
#   - Allow multiple urls. Each one is tried before failing
# Release 1.4   Burn Alting 20190203
#   - Adjusted comment about Solaris timezones. Note .. non functional change!
# Release 1.3   Burn Alting 20190131
#   - Allow for the collection from a single identified file
#   - Allow the periodicity of execution delay to be specified
#   - Allow to specify lock file
#   - Allow to specify checkpoint file
#   - Cater for bug in ausearch (wef 20190120 - audit-3.0-0.5.20181218gitbdb72c0.fc29) where running
#     ausearch --checkpoint on a single file does not update dev/inode fields of checkpoint file
#   - Gain host's Canonical Timezone TZ database name (Australia/Sydney, Europe/London, etc - https://www.iana.org/time-zones) and pass in post
#   - Cater for poorly configured name server resolution
# Release 1.2   Burn Alting 20180802
#   - Protected the conversion of space characters to slashes in generating the VERSION string
# Release 1.1   Burn Alting 20180411
#   - Corrected minor script errors
# Release 1.0   Burn Alting 20170520
#   - Initial Release

# This script 
#   - on start up delays for a random period of time before proceeding. This is intended to
#     inject random transmission load across a network of many systems generating audit.
#   - determines the current audit release and supporting tools,
#   - generates recent audit activity enriching the raw auditd data to make it more useful
#   - compresses the generated data and queues same for transmission
#   - transmits the compressed data to a Stroom audit proxy. Should the transmission
#     fail, their re-transmission will occur on the next script invocation.
#   - optionally the compressed data is encapsulated into a 'disconnect package file' 
#     (which can subsequently be posted to a Stroom proxy)
#
# To protect against disk overflow, the queue directory will age off old log files based on time and maximum file size.

# Usage:
#   stroom_auditd_feeder.sh [-fnP] [-F auditdlogfile] [-l LockFile] [-d delaysecs] [-c checkpointfile] [-R days] [-M blockcount] [-p disconectfilesdir]
#   -f         force the run.
#   -n         prevents the random sleep prior to processing
#   -P         form a 'disconnect package file' rather than post the data to a stroom proxy
#   -p         specify a directory to look for 'disconnect package files' and post them
#   -F fn      specify the raw audit log file to gain logs from
#   -l fn      specify a unique lock file name, so multiple invocations of this script can exist. If a path then this is the lock file
#   -d secs    specify the MAX_SLEEP value (default is 590)
#   -c fn      specify the checkpoint file
#   -R         set a different failed retention period
#   -M         set a different failed maximum size to be retained until age off

#
Usage="Usage: $(basename $0) [-nfP] [-F auditdlogfile] [-l LockFile] [-d max_sleep] [-c checkpointfile] [-R days] [-M blockcount] [-p disconectfilesdir]"

# -------------------
# ARGUMENT PROCESSING:
# -------------------
Arg0=$(basename $0)
THIS_PID=$(echo $$)

# We should normally sleep before processing data
NoSleep=0

# We normally don't delay between posts
NoDelay=0

# We do not force the execution by default
Force=0

# We do not, by default, form a disconnect package file
DoNotPost=0

# Set maxsleep
Max_sleep=''

# Lock file
LockFile=''

# Filename template
AuditLogFile=''

# Checkpoint file
CheckpointFile=''

# Disconnect Package File directory - if set, scan and post
DisconnectPackageDir=''

# New FAILED_MAX value
NewFAILED_MAX=''
# New FAILED_RETENTION value
NewFAILED_RETENTION=''

# Check args
while getopts "fnPc:d:l:F:R:M:p:" opt; do
  case $opt in
  F)
    AuditLogFile=$OPTARG
    ;;
  f)
    Force=1
    ;;
  n)
    NoSleep=1
    ;;
  P)
    DoNotPost=1
    ;;
  c)
    CheckpointFile=$OPTARG
    ;;
  l)
    LockFile=$OPTARG
    ;;
  d)
    Max_sleep=$OPTARG
    ;;
  R)
    NewFAILED_RETENTION=$OPTARG
    ;;
  M)
    NewFAILED_MAX=$OPTARG
    ;;
  p)
    DisconnectPackageDir=$OPTARG
    ;;
  \?)
    echo "$0: Invalid option -$OPTARG"
    echo $Usage
    exit 1
    ;;
 esac
done

# We need to be root to access the log files
if [ $EUID -ne 0 ]; then
  echo "$0: Must run as root"
  exit 1
fi

# Check logfile conditions
if [ -n "${AuditLogFile}" ]; then
  if [ ! -f ${AuditLogFile} ]; then
    echo "$0: Logfile must exist. Logfile - ${AuditLogFile}"
    exit 1
  fi
  if [ ! -n "${LockFile}" ]; then
    echo "$0: If you specify a log file, you must also specify a lock file"
    exit 1
  fi
fi

# Checkpoint file checks
# Although the file doesn't have to exist, it's path does
if [ -n "${CheckpointFile}" ] && [ ! -d $(dirname $CheckpointFile) ]; then
  echo "$0: Checkpoint path must exist - $(dirname $CheckpointFile)"
  exit 1
fi

# Check disconnect package directory if given.
if [ -n "${DisconnectPackageDir}" ] && [ ! -d ${DisconnectPackageDir} ]; then
  echo "$0: No such directory - ${DisconnectPackageDir}"
  exit 1
fi

# -----------------------------------
# CONFIGURATION environment variables
# -----------------------------------

# SYSTEM            - Server System type
#
# For Linux Operating system logs can be LinuxServer, LinuxWorkstation
# We test for an operating X interface. This is very approximate test.
SYSTEM="LinuxServer"
if command -v ss &> /dev/null; then
  command ss -lptx | grep -iF X11 > /dev/null
  if [ $? -eq 0 ]; then
    SYSTEM="LinuxWorkstation"
  fi
fi

# ENVIRONMENT       - Application environment
#
# Can be  Production, QualityAssurance or Development
ENVIRONMENT="Production"

# URL               - URL for posting gzip'd audit log files
#                     Multiple comma separated urls can be provided. The script will attempt to send
#                     to each url until success. The curl connection timeout is automatically lowered
#                     to 10 seconds if there are multiple urls to reduce the time to attempt posting
#
# This should NOT change without consultation with Audit Authority
URL=https://stroom-proxy00.somedomain.org/stroom/datafeed,https://stroom-proxy07.somedomain.org/stroom/datafeed

# mySecZone - Security zone if pertinent
#
# We set this typically externally to the base script script source
mySecZone="none"

# VERSION           - The version of the log source
#
# This is to allow one to distinguish between different versions of the capability
# generating logs. If you have strong version control on the logging element of
# your application, you can use your release of the installed utility version.
# Samples are
#   Basic extraction from a rpm package
#       VERSION=$(rpm -q httpd)
#   Complex combination of OS version, auditd package and indication of stroom auditd package
# 20231206:
# We establish a version
AgentVersion="stroom_auditd_agent_3.1.4"
if command -v rpm > /dev/null 2>&1; then
  rpm -q stroom_auditd_agent > /dev/null 2>&1
  if [ $? -eq 0 ]; then
    _pkg=$(rpm -q audit stroom_auditd_agent | tr '[:space:]' /)
  else
    _pkg=$(rpm -q audit | tr '[:space:]' /)${AgentVersion}/
  fi
elif command -v dpkg-query > /dev/null 2>&1; then
  _pkg=$(dpkg --status auditd | awk '{if ($1 == "Version:") print $2;}')/${AgentVersion}/
else
  _pkg="UnknownPackageManager/${stroom_auditd_agent}/"
fi
VERSION=$(uname -r)/${_pkg}


# FEED_NAME         - Name of Stroom feed (assigned by Audit Authority)
#
# We work out which feed we want based on the Linux audit version running and possible
# supporting toolsets
FEED_NAME="LINUX-AUDITD-AUSEARCH-V3-EVENTS"

# STROOM_ROOT - The base or working directory for log collection and processing
#
# Note: If you are managing multi log sources, then ensure this directory is
# unique for each source of log as files that fail to transmit are placed
# in a holding directory in this base directory (${STROOM_ROOT}/failed) and each
# source of logs has it's own unique feed names
STROOM_ROOT=_AUDIT_ROOT_

# 20230903:
# To support those orchestration systems that just want to use the generic
# location of /opt/stroom/auditd we do a simple test here. This, in combination with
# the configuration file means one can deploy the base stroom_auditd_feeder.sh.base file.

if [[ "${STROOM_ROOT}" == "_AUDIT_ROO"* ]]; then
  STROOM_ROOT=/opt/stroom/auditd
fi

# 
# Working directories 
# STROOM_LOG_QUEUED    - Directory to queue logs ready for transmission
STROOM_LOG_QUEUED=${STROOM_ROOT}/queue
# LOCK DIRECTORY for lock and checkpoint files
STROOM_CLIENT_LOCKS=${STROOM_ROOT}/locks
# CONFIG_FILE - Stroom configuration file, if present, is sourced
STROOM_CONFIG_FILE=${STROOM_ROOT}/config

# If we have a LockFile then we use this as the lock file, else it's a template
if [ -n "$LockFile" ]; then
  LCK_FILE=$LockFile
else
  LCK_FILE=${STROOM_CLIENT_LOCKS}/$Arg0.lck
fi

# STROOM_CHECKPOINT_FILE - File used to maintain ausearch state/checkpoint information
if [ -n "${CheckpointFile}" ]; then
  STROOM_CHECKPOINT_FILE=${CheckpointFile}
else
  STROOM_CHECKPOINT_FILE=${STROOM_CLIENT_LOCKS}/auditd_checkpoint.txt
fi

if [ -f /sbin/ausearch ]; then
  Processor=/sbin/ausearch
elif [ -f /usr/sbin/ausearch ]; then
  Processor=/usr/sbin/ausearch
else
  logmsg "Cannot find ausearch binary"
  exit 1
fi

# Do we have aushape support? Aushape is a new Linux audit capability
# that can generate JSON or XML variant of Linux auditd data
# Note the the --input-logs argument is mandatory, for ausearch checks stdin to see
# if it is a pipe. If so, it reads from the pipe not the log files. Now, cron jobs always
# creates a pipe for stdin, even when it doesn't pipe anything to it. Thus the need for
# --input-logs as this script is invariably run from cron.
if hash aushape 2>/dev/null; then
  FEED_NAME="LINUX-AUDITD-AUSHAPE-V3-EVENTS"
  if [ -n "${AuditLogFile}" ]; then
    ProcessorArgs="-if ${AuditLogFile} --raw --checkpoint ${STROOM_CHECKPOINT_FILE}"
  else
    ProcessorArgs="--raw --input-logs --checkpoint ${STROOM_CHECKPOINT_FILE}"
  fi
  ProcessorRecoverArgs="${ProcessorArgs} --start checkpoint"
  Process="C"   # indicate ausearch/aushape processing
else
  if [ -n "${AuditLogFile}" ]; then
    ProcessorArgs="-i -if ${AuditLogFile} --checkpoint ${STROOM_CHECKPOINT_FILE}"
  else
    ProcessorArgs="-i --input-logs --checkpoint ${STROOM_CHECKPOINT_FILE}"
  fi
  ProcessorRecoverArgs="${ProcessorArgs} --start checkpoint"
  Process="A"   # indicate ausearch processing
fi

# FAILED_RETENTION  - Retention period to hold logs that failed to transmit (days)
#
# This period, in days, is to allow a log source to temporarily maintain local copies
# of failed to transmit logs.
FAILED_RETENTION=90
if [ -n "${NewFAILED_RETENTION}" ]; then
  FAILED_RETENTION=${NewFAILED_RETENTION}
fi

# FAILED_MAX        - Specify a storage limit on logs that failed to transmit (512-byte blocks)
#
# As well as a retention period for logs that failed to transmit, we also
# limit the size of this archive in terms byte.
# The value is in 512-byte blocks rather than bytes
# For example, 1GB is 
#   1 GiB = 1 * 1024 * 2048 = 2097152
#   8 GiB = 8 * 1024 * 2048 = 16777216
FAILED_MAX=16777216
if [ -n "${NewFAILED_MAX}" ]; then
  FAILED_MAX=${NewFAILED_MAX}
fi

# MAX_SLEEP         - Time to delay the processing and transmission of logs
#
# To avoid audit logs being transmitted from the estate at the same time, we will
# delay a random number of seconds up to this maximum before processing and
#
# This value should NOT be changed with permission from Audit Authority. It should
# also be the periodicity of the calling of the feeding script. That is, cron
# should call the feeding script every MAX_SLEEP seconds
if [ -n "${Max_sleep}" ]; then
MAX_SLEEP=$Max_sleep
else
MAX_SLEEP=590
fi

# C_TMO             - Maximum time in seconds to allow the connection to the server to take
C_TMO=37

# If we have a configuration file, source it
if [ -f ${STROOM_CONFIG_FILE} ]; then
  source ${STROOM_CONFIG_FILE}
fi

# Split if we have multiple URLS (comma separated) into the array urls and note it's size
IFS="," read -ra urls <<< "${URL}"
urls_idx=0
urls_mod=${#urls[@]}

# We lower the C_TMO value if we have multiple urls
if [ $urls_mod -gt 1 ]; then
C_TMO=10
fi

# M_TMO             - Maximum time in seconds to allow each curl operation to take
M_TMO=1200

# PostFailureMax    - Number of attempts to post files in the queue before we give up
PostFailureMax=3

# --------
# ROUTINES:
# --------

# clean_store()
# Args:
#  $1 - root      - the root of the archive directory to clean
#  $2 - retention - the retention period in days before archiving
#  $3 - maxsize   - the maximum size in (512-byte) blocks allowed in archive
#
# Ensure any local archives of logs are limited in size and retention period

clean_store()
{
  if [ $# -ne 3 ] ; then
    echo "$Arg0: Not enough args calling clean_archive()"
    return
  fi
  root=$1
  retention=$2
  maxsize=$3

  # Just to be paranoid
  if [ ${root} = "/" ]; then
      echo "$Arg0: Cannot clean_archive root filesystem"
      return
  fi
  # Make sure it's there
  if [ ! -d ${root} ]; then
      echo "$Arg0: Attempting to clean missing directory - ${root}"
      return
  fi

  # We first delete files older than the retention period
  find ${root} -type f -mtime +${retention} -delete

  # First cd to ${root} so we don't need shell expansion on
  # the ls command below.
  myloc=$(pwd)
  cd ${root}
  # We next delete based on the max size for this store
  s=$(du -s --block-size=512 . | cut -f1)
  _protect_loop=0
  while [[ ${s} -gt ${maxsize} && ${_protect_loop} -lt 200 ]]; do
    ls -t | tail -5 | xargs rm -f
    s=$(du -s --block-size=512 . | cut -f1)
    ((_protect_loop++))
  done
  cd ${myloc}
  return
}

# logmsg()
# Args:
#   $* - arguments to echo
#
# Print a message prefixed with a date and the program name

logmsg() {
  NOW=$(date +"%FT%T.000%:z")
  echo "${NOW} ${Arg0} $(hostname): $*"
}

# stroom_get_lock()
# Args:
#   none
#
# Obtain a lock to prevent duplicate execution
stroom_get_lock() {

  if [ -f "${LCK_FILE}" ]; then
    MYPID=$(head -n 1 "${LCK_FILE}")
    TEST_RUNNING=$(ps -p ${MYPID} | grep -F ${MYPID})

    if [ -z "${TEST_RUNNING}" ]; then
      logmsg "Obtained lock for ${THIS_PID}"
      echo "${THIS_PID}" > "${LCK_FILE}"
    else
      logmsg "Sorry ${Arg0} is already running[${MYPID}]"
      # If the lock file is over thee hours old remove it. Basically remove clearly stale lock files
      find ${LCK_FILE} -mmin +180 -delete
      exit 0
    fi
  else
    logmsg "Obtained lock for ${THIS_PID} in ${LCK_FILE}"
    echo "${THIS_PID}" > "${LCK_FILE}"
  fi
}

# stroom_try_lock()
# Args:
#   none
#
# See if we can get a lock
stroom_try_lock() {

  if [ -f "${LCK_FILE}" ]; then
    MYPID=$(head -n 1 "${LCK_FILE}")
    TEST_RUNNING=$(ps -p ${MYPID} | grep -F ${MYPID})

    if [ -z "${TEST_RUNNING}" ]; then
      logmsg "Obtained lock for ${THIS_PID}"
      echo "${THIS_PID}" > "${LCK_FILE}"
      return 0
    else
      logmsg "Sorry ${Arg0} is already running[${MYPID}]"
      find ${LCK_FILE} -mmin +180 -delete
      return 1
    fi
  else
    logmsg "Obtained lock for ${THIS_PID} in ${LCK_FILE}"
    echo "${THIS_PID}" > "${LCK_FILE}"
    return 0
  fi
}

# stroom_rm_lock()
# Args:
#   none
#
# Remove lock file

stroom_rm_lock() {
  if [ -f ${LCK_FILE} ]; then
    logmsg "Removed lock ${LCK_FILE} for ${THIS_PID}"
    rm -f ${LCK_FILE}
  fi
}

# gain_iana_timezone()
# Args:
#   null
#
# Gain the host's Canonical timezone
# The algorithm in general is
#   if /etc/timezone then
#     This is a ubuntu scenario
#     cat /etc/timezone
#   elif /etc/localtime is a symbolic link and /usr/share/zoneinfo exists
#     # This is a RHEL/BSD scenario. Get the filename in the database directory
#     readlink /etc/localtime | sed -e 's@.*share/zoneinfo/\(posix\|right\|leaps\)/\|.*share/zoneinfo/@@'
#   elif /etc/localtime is a file and /usr/share/zoneinfo exists
#     # This is also a RHEL/BSD scenario. Get the filename in the database directory by brute force comparison
#     find /usr/share/zoneinfo -type f ! -name 'posixrules' -execdir cmp -s {} /etc/localtime \; -print | sed -e 's@.*share/zoneinfo/\(posix\|right\|leaps\)/\|.*share/zoneinfo/@@' | head -n1
#   elif /etc/TIMEZONE exists
#     # This is for Solaris for completeness. Get the TZ value. May need to delete double quotes
#     grep 'TZ=' /etc/TIMEZONE | cut -d= -f2- | sed -e 's/"//g'
#   else
#     nothing
#
gain_iana_timezone()
{
  if [ -f /etc/timezone ]; then
    # Ubuntu based
    cat /etc/timezone
  elif [ -h /etc/localtime -a -d /usr/share/zoneinfo ]; then
    # RHEL/BSD based
    readlink /etc/localtime | sed -e 's@.*share/zoneinfo/\(posix\|right\|leaps\)/\|.*share/zoneinfo/@@'
  elif [ -f /etc/localtime -a -d /usr/share/zoneinfo ]; then
    # Older RHEL based
    find /usr/share/zoneinfo -type f ! -name 'posixrules' -execdir cmp -s {} /etc/localtime \; -print | sed -e 's@.*share/zoneinfo/\(posix\|right\|leaps\)/\|.*share/zoneinfo/@@' | head -n1
  fi
}

# send_to_stroom()
# Args:
#  $1 - the log file
#
# Send the given log file to the Stroom Web Service (if DoNotPost == 0)
# Note we use the 'remove trailing space construct' when assigning variables to httpd header variables
# ie ${var%"${var##*[![:space:]]}"} otherwise, curl gets unexpected arguments

send_to_stroom() {
  logFile=$1
  logSz=$(ls -sh ${logFile} | cut -d' ' -f1)

  # Create an array of metadata for transmission.
  # If this is a 'disconnect package file' post, then there will be a .meta file with the metadata already
  # formed. If not, we form it
  declare -a hostArgs=()
  if [ -f ${logFile}.meta ]; then
    while IFS='' read -r -d '' item; do
      hostArgs+=( "$item" )
    done < ${logFile}.meta
  else
    # Create an array of local metadata for transmission.
    hostArgs+=(-H "Feed:${FEED_NAME}" -H "System:${SYSTEM}" -H "Environment:${ENVIRONMENT}" -H "Version:${VERSION}")
    if [ -n "${mySecZone}" -a "${mySecZone}" != "none" ]; then
      hostArgs+=(-H "MySecurityDomain:\"${mySecZone%"${mySecZone##*[![:space:]]}"}\"")
    fi

    hostArgs+=(-H "Shar256:$(sha256sum -b ${logFile} | cut -d' ' -f1)" -H "LogFileName:$(basename ${logFile})")
    myHost=$(hostname --all-fqdns 2> /dev/null)
    if [ $? -ne 0 ]; then
      myHost=$(hostname)
    fi
    myIPaddress=$(hostname --all-ip-addresses 2> /dev/null)
    hostArgs+=(-H "MyHost:\"${myHost%"${myHost##*[![:space:]]}"}\"")
    hostArgs+=(-H "MyIPaddress:\"${myIPaddress%"${myIPaddress##*[![:space:]]}"}\"")

    myDomain=$(hostname -d 2>/dev/null)
    if [ -n "${myDomain}" ]; then
      myNameserver=$(dig ${myDomain} SOA +time=3 +tries=2 +noall +answer +short 2>/dev/null | head -1 | cut -d' ' -f1)
      if [ -n "$myNameserver" ]; then
        hostArgs+=(-H "MyNameServer:\"${myNameserver}\"")
      else
        # Let's try dumb and see if there is a name server in /etc/resolv.conf and choose the first one
        h=$(grep -E '^nameserver ' /etc/resolv.conf | head -1 | cut -f2 -d' ')
        if [ -n "${h}" ]; then
          h0=$(host $h 2> /dev/null)
          if [ $? -eq 0 -a -n "${h0}" ]; then
             hostArgs+=(-H "MyNameServer:\"$(echo $h0 | gawk '{print $NF }')\"")
          elif [ -n "${h}" ]; then
             hostArgs+=(-H "MyNameServer:\"${h}\"")
          fi
        fi
      fi
    fi
    # Gather various configuration details via facter(1) command if available
    if hash facter 2>/dev/null; then
      # Redirect facter's stderr as we may not be root
      myMeta=$(facter 2>/dev/null | awk '{
  if ($1 == "fqdn") printf "FQDN:%s\\\n", $3;
  if ($1 == "uuid") printf "UUID:%s\\\n", $3;
  if ($1 ~ /^ipaddress/) printf "%s:%s\\\n", $1, $3;
  }')
      if [ -n "${myMeta}" ]; then
        hostArgs+=(-H "MyMeta:\"${myMeta}\"")
      fi
    fi
    # Local time zone
    ltz=$(date +%z)
    if [ -n "${ltz}" ]; then
        hostArgs+=(-H "MyTZ:${ltz}")
    fi

    # Local Canonical Timezone
    ctz=$(gain_iana_timezone)
    if [ -n "${ltz}" ]; then
        hostArgs+=(-H "MyCanonicalTZ:${ctz}")
    fi

    # Gain health items (replace space with ^g to make curl arg simpler. Stroom translation can correct
    health0="VarLogAudit^g$(df -h /var/log/audit| tail -1 | sed -e 's/ /\^g/g')"
    health1="STROOM_ROOT^g$(df -h $STROOM_ROOT | tail -1 | sed -e 's/ /\^g/g')"
    if [ -n "${health0}" ]; then
      hostArgs+=(-H "Health0:\"${health0}\"")
    fi
    if [ -n "${health1}" ]; then
      hostArgs+=(-H "Health1:\"${health1}\"")
    fi

    hostArgs+=(-H "Compression:GZIP")
  fi

  # Do the transfer.
  # If we post, then we loop through the urls array. We use the index urls_idx to iterate over the array
  # this way, if we have a few failures to post, then the index will be at the successful
  # url if we have multiple files to post
  # If we do not post we form a 'disconnect package file' containing the original file and the post metadata (excluding url)

  if [ ${DoNotPost} -eq 0 ]; then
    u=${urls[$urls_idx]}
    _i=0
    while [ $_i -lt $urls_mod ]; do

      # For two-way SSL authentication replace '-k' below with '--cert /path/to/server.pem --cacert /path/to/root_ca.crt' on the curl cmds below
      # If not two-way SSL authentication, use the -k option to curl

      RESPONSE_HTTP=$(curl -k --connect-timeout ${C_TMO} --max-time ${M_TMO} --data-binary @${logFile} ${u} "${hostArgs[@]}" \
        --write-out "RESPONSE_CODE=%{http_code}" 2>&1)
      _err=$?
    
      # We first look for a positive response (ie 200)
      RESPONSE_CODE=$(echo ${RESPONSE_HTTP} | sed -e 's/.*RESPONSE_CODE=\(200\).*/\1/')
      if [ "${RESPONSE_CODE}" = "200" ] ;then
        logmsg "Send status: [${RESPONSE_CODE}] SUCCESS  Audit Log: ${logFile} Size: ${logSz} ProcessTime: ${ProcessTime} Feed: ${FEED_NAME} URL: ${u}"
        rm -f ${logFile}
        rm -f ${logFile}.meta
        return 0
      fi
    
      # If we can't find it in the output, look for the last response code
      # We do this in the unlikely event that a corrupted argument is passed to curl
      RESPONSE_CODE=$(echo ${RESPONSE_HTTP} | sed -e 's/.*RESPONSE_CODE=\([0-9]\+\)$/\1/')
      if [ "${RESPONSE_CODE}" = "200" ] ;then
        logmsg "Send status: [${RESPONSE_CODE}] SUCCESS  Audit Log: ${logFile} Size: ${logSz} ProcessTime: ${ProcessTime} URL: ${u}"
        rm -f ${logFile}
        rm -f ${logFile}.meta
        return 0
      fi
    
      # Fall through ...
    
      # We failed to transfer the processed log file, so emit a message to that effect
      msg="Send status: [${RESPONSE_CODE}] FAILED  Audit Log: ${logFile} Reason: curl returned http_code (${RESPONSE_CODE}) and error code ${_err} URL: ${u}"
      logmsg "$msg"

      # Simple check for a corrupt ${logFile}
      _egz=$(gunzip -t ${logFile} 2>&1)
      if [ $? -ne 0 ]; then
        logmsg "Compressed file ${logFile} is corrupt - ${_egz}. Removing"
        rm -f ${logFile}
        rm -f ${logFile}.meta
        return 1
      fi

      # Work out the next url to use
      ((_i++))
      urls_idx=$((++urls_idx % urls_mod))
      u=${urls[$urls_idx]}
    done
  else
    # We need to form a 'disconnect package file'
    _emeta=$(basename ${logFile}).meta
    _dpf=E_$(basename  ${logFile} .gz).tar
    # Emit the hostArgs to the metadata file
    printf '%s\0' "${hostArgs[@]}" > ${_emeta}
    # Form the 'disconnect package file'
    mv ${logFile} $(basename ${logFile}).data
    tar cf ${_dpf} ${_emeta} $(basename ${logFile}).data
    _err=$?
    if [ ${_err} -eq 0 ]; then
      logmsg "Created disconnect package file Audit Log: ${logFile} Size: ${logSz} ProcessTime: ${ProcessTime} Feed: ${FEED_NAME} PkgFn: ${_dpf}"
      rm -f ${logFile}.data ${_emeta}
      return 0
    else
      msg="Failed to tar up disconnect package file: ${_dpf} with ${_emeta} ${logFile}.data. Err ${_err}"
      logmsg "$msg"
      mv ${logFile}.data ${logFile}
      rm -f ${_emeta}
    fi
  fi
   
  # We also send an event into the security syslog destination
  logger -p "authpriv.info" -t $Arg0 "$msg"

  return 9
}

# process_ausearch()
# Args:
#  $1 - the file template to write to
#
# Process the auditd logs using ausearch
# This function
#  - ensures all logs have a node=<hostname> key-value pair on every line in case auditd.conf is misconfigured/overwritten
#  - we set LC_TIME="en_DK" to get an ISO8601 format YYYY-MM-DD hh:mm:ss.msec and we set LC_ALL="" to ensure
#    LC_TIME is not ignored if LC_ALL is set to something
#  - we set TZ=UTC to ensure the date/time is consistent - ie UTC timezone as there
#    is no timezone in the ausearch date timestamp.
process_ausearch()
{
  f=$1.tmp
  # See if we have to insert node names due to bad configuration
  _need_node=0
  if [ -n "${AuditLogFile}" ]; then
    _tf=$AuditLogFile
  else
    _tf=/var/log/audit/audit.log
  fi
  head -2 ${_tf} | grep -E '^type=' > /dev/null
  if [ $? -eq 0 ]; then
    _need_node=1
  fi

  if [ "${Process}" = "C" ]; then
    if [ ${_need_node} -eq 1 ]; then
      LC_ALL="" LC_TIME="en_DK" TZ=UTC ${Processor} ${ProcessorArgs} 2> /tmp/ausearch.out.$$ | \
        awk -v H=$(hostname --fqdn) '{ if (/^type=/) { printf "node=%s %s\n", H, $0;} else { print $0;} }' | \
        aushape --lang=xml | gzip -f > $1.gz
    else
      LC_ALL="" LC_TIME="en_DK" TZ=UTC ${Processor} ${ProcessorArgs} 2> /tmp/ausearch.out.$$ | \
        aushape --lang=xml | gzip -f > $1.gz
    fi
  else
    if [ ${_need_node} -eq 1 ]; then
      LC_ALL="" LC_TIME="en_DK" TZ=UTC ${Processor} ${ProcessorArgs} 2> /tmp/ausearch.out.$$ | \
        awk -v H=$(hostname --fqdn) '{ if (/^type=/) { printf "node=%s %s\n", H, $0;} else { print $0;} }' | \
        gzip -f > $1.gz
    else
      LC_ALL="" LC_TIME="en_DK" TZ=UTC ${Processor} ${ProcessorArgs} 2> /tmp/ausearch.out.$$ | \
        gzip -f > $1.gz
    fi
  fi
  _fstatus=${PIPESTATUS[0]}

  # On first failure, try recovery
  if [ $_fstatus -eq 10 -o $_fstatus -eq 11 -o $_fstatus -eq 12 ]; then
    rm -f $1.gz # Remove the file
    logmsg ${Processor} ${ProcessorArgs} failed status ${_fstatus}. Attempting recovery.
    if [ "${Process}" = "C" ]; then
      if [ ${_need_node} -eq 1 ]; then
        LC_ALL="" LC_TIME="en_DK" TZ=UTC ${Processor} ${ProcessorRecoverArgs} 2> /tmp/ausearch.out.$$ | \
          awk -v H=$(hostname --fqdn) '{ if (/^type=/) { printf "node=%s %s\n", H, $0;} else { print $0;} }' | \
          aushape --lang=xml | gzip -f > $1.gz
      else
        LC_ALL="" LC_TIME="en_DK" TZ=UTC ${Processor} ${ProcessorRecoverArgs} 2> /tmp/ausearch.out.$$ | \
          aushape --lang=xml | gzip -f > $1.gz
      fi
    else
      if [ ${_need_node} -eq 1 ]; then
        LC_ALL="" LC_TIME="en_DK" TZ=UTC ${Processor} ${ProcessorRecoverArgs} 2> /tmp/ausearch.out.$$ | \
          awk -v H=$(hostname --fqdn) '{ if (/^type=/) { printf "node=%s %s\n", H, $0;} else { print $0;} }' | \
          gzip -f > $1.gz
      else
        LC_ALL="" LC_TIME="en_DK" TZ=UTC ${Processor} ${ProcessorRecoverArgs} 2> /tmp/ausearch.out.$$ | \
          gzip -f > $1.gz
      fi
    fi
    _nstatus=${PIPESTATUS[0]}
    # If both executions return 10 - invalid checkpoint data found in checkpoint file
    # we log it's content then remove it
    if [ $_nstatus -ne 0 ]; then
      rm -f $1.gz # Remove the file
      if [ $_fstatus -eq 10 -a $_nstatus -eq 10 ]; then
        logmsg ${Processor} ${ProcessorRecoverArgs} failed. Removing checkpoint file. Status: 10 FileContent: $(paste -s ${STROOM_CHECKPOINT_FILE} -d:)
        rm -f ${STROOM_CHECKPOINT_FILE}
      else
        logmsg ${Processor} ${ProcessorRecoverArgs} failed $(cat /tmp/ausearch.out.$$)
      fi
      logger -p "user.info" -t stroom_auditd_feeder.sh "stroom_auditd Failed ${Processor} status ${_nstatus}"   
    fi
    _status=${_nstatus}
  else
    _status=${_fstatus}
  fi
  # If we have a specific log file and valid status, then update the checkpoint file
  # When using a specific file, the checkpoint code has a bug in that it does not
  # record device/inode in the checkpoint file. This bug is reported in 
  # https://bugzilla.redhat.com/show_bug.cgi?id=1663285.
  # TODO: Until the bug is fixed, we need to place the device/inode in the checkpoint file
  # The following will need to be revisited based on auditd release but for the present
  # we just test for dev=0x0 and inode=0 in the checkpoint file and add them
  # but only if we are specifying a file
  if [ -n "${AuditLogFile}" ] && [ -f ${STROOM_CHECKPOINT_FILE} ]; then
    grep -E '^dev=0x0$|^inode=0$' ${STROOM_CHECKPOINT_FILE} > /dev/null
    if [ $? -eq 0 ]; then
      stat --printf=dev=0x%D\\ninode=%i\\n ${AuditLogFile} |
        sed -i -e '1,2{R /dev/stdin' -e 'd}' ${STROOM_CHECKPOINT_FILE}
    fi
  fi
  rm -f /tmp/ausearch.out.$$
  if [ -f $1.gz ]; then
    chmod 600 $1.gz
  fi
  return $_status
}

# ----
# MAIN:
# ----

# Set up a delay of between 7 - $MAX_SLEEP seconds
# The additional 7 seconds is to allow for log acquisition time

RANDOM=$(echo ${RANDOM})
MOD=$(expr ${MAX_SLEEP} - 7)
SLEEP=$(expr \( ${RANDOM} % ${MOD} \) + 7)

# Check for existence of working directories
if [ ! -d ${STROOM_LOG_QUEUED} ]; then
    logmsg "Cannot find Queue directory ${STROOM_LOG_QUEUED}. Will create."
    mkdir -p ${STROOM_LOG_QUEUED} && chmod 700 ${STROOM_LOG_QUEUED}
    if [ $? -ne 0 ]; then
      logmsg "Cannot mkdir Queue directory  ${STROOM_LOG_QUEUED}"
      exit 1
    fi
fi
if [ ! -d ${STROOM_CLIENT_LOCKS} ]; then
    logmsg "Cannot find Locks/Checkpoint directory ${STROOM_CLIENT_LOCKS}. Will create."
    mkdir -p ${STROOM_CLIENT_LOCKS} && chmod 700 ${STROOM_CLIENT_LOCKS}
    if [ $? -ne 0 ]; then
      logmsg "Cannot mkdir Locks/Checkpoint directory ${STROOM_CLIENT_LOCKS}"
      exit 1
    fi
fi

# Ensure disconnect package directory is not the same as the queue directory
if [ -n "${DisconnectPackageDir}" ] && [ ${DisconnectPackageDir} == ${STROOM_LOG_QUEUED} ]; then
  logmsg "Disconnect package directory ${DisconnectPackageDir} cannot be the same as the Stroom Log Queue directory ${STROOM_LOG_QUEUED}"
  exit 1
fi

# Do we force a run
if [ ${Force} -ne 0 ]; then
  logmsg "Forcing executing of stroom_auditd_agent script"
  # See if we can get a lock
  stroom_try_lock
  if [ $? -eq 1 ]; then
    logmsg "Forcing executing of stroom_auditd_agent script - killing existing execution"
    kill $(cat ${LCK_FILE})
    sleep 2
  fi
else
  # Get a lock
  stroom_get_lock
fi

# We may need to sleep
if [ ${NoSleep} -eq 0 ]; then
  logmsg "Will sleep for ${SLEEP}s to help balance network traffic"
  sleep ${SLEEP}
fi


if [ -z "${DisconnectPackageDir}" ]; then
  # We now process the logs from the source and store the processed logs in our queuing directory
  # Generate a unique file for this batch
  GenLog="${STROOM_LOG_QUEUED}/auditdProcessed.${THIS_PID}.$(date +%s)"
  logmsg "Start gathering audit into ${GenLog}.gz"

  process_ausearch ${GenLog}

  # If ausearch fails, we tidy up the file, but fall through in order to transmit, possibly, queued files
  if [ $? -ne 0 ]; then
    rm -f ${GenLog}.gz
  fi
else
  # We have a disconnect package directory, lets see if it has files
  _myloc=$(pwd)
  cd ${DisconnectPackageDir}
  l=$(find . -regextype posix-extended -regex './E_auditdProcessed.[0-9]+.[0-9]+.tar$')
  if [ -n "${l}" ]; then
    # Create a temporary working directory below the ${DisconnectPackageDir}
    DisconnectPackageDirWorking=${DisconnectPackageDir}/_working
    if [ ! -d ${DisconnectPackageDirWorking} ]; then
      mkdir -p ${DisconnectPackageDirWorking}
    fi
    for f in $l; do
      tar -xf $f --directory ${DisconnectPackageDirWorking}
      if [ $? -ne 0 ]; then
        # We exit if we fail to extract in order to preserve the disconnect package file
        logmsg "Failed to extract disconnect package $f into ${DisconnectPackageDirWorking}"
        stroom_rm_lock
        exit 1
      fi
      # OK, we have tar'd out the contents
      # the meta file - auditdProcessed.N.dtg.gz.meta, and
      # the data file - auditdProcessed.N.dtg.gz.data
      _myloc2=$(pwd)
      cd ${DisconnectPackageDirWorking}
      for k in *.meta; do
        _d=$(basename $k .meta)
        if [ ! -f ${_d}.data ]; then
          logmsg "Disconnected meta file $k but not data file - ${_d}.data"
          stroom_rm_lock
          exit 1
        fi
        # We move both the data and meta files to the QUEUE directory
        mv ${_d}.data ${STROOM_LOG_QUEUED}/${_d}
        if [ $? -ne 0 ]; then
          logmsg "Failed to move disconnected data file ${_d}.data"
          stroom_rm_lock
          exit 1
        fi
        mv ${k} ${STROOM_LOG_QUEUED}/${k}
        if [ $? -ne 0 ]; then
          logmsg "Failed to move disconnected meta file ${k}"
          stroom_rm_lock
          exit 1
        fi
      done
      cd $_myloc2
      # If we are here, then we have successfully moved the contents of this disconnect package
      # file to ${STROOM_LOG_QUEUED} so we can remove it
      rm -f $f
    done
    # Remove the temporary working directory if empty
    if [ -z "$(ls -A ${DisconnectPackageDirWorking})" ]; then
      rm -rf ${DisconnectPackageDirWorking}
    else
      logmsg "Disconnect package working directory, ${DisconnectPackageDirWorking}, is not empty. Cannot remove"
    fi
  else
    logmsg "Disconnect package directory, ${DisconnectPackageDir}, was empty. No files processed"
  fi
  # Return to base location
  cd ${_myloc}
fi

# Remember our location
_myloc=$(pwd)
cd ${STROOM_LOG_QUEUED}
# Gzip any non-gziped files
l=$(find . -regextype posix-extended -regex './auditdProcessed.[0-9]+.[0-9]+$')
if [ -n "${l}" ]; then
  for f in $l; do
    if [ -s $f ]; then
      gzip --force $f
    else
      # Delete empty files
      logmsg "Processing file $f was empty"
      rm -f $f
    fi
  done
fi

# Now post all gzip'd files

l=$(find . -regextype posix-extended -regex './auditdProcessed.[0-9]+.[0-9]+.gz$')
if [ -n "${l}" ]; then
  _setnfails=0
  for f in $l; do
    if [ -s ${f} ]; then
      export ProcessTime=0
      send_to_stroom ${f}
      # Count failure to post files
      if [ $? -eq 9 ]; then
        ((_setnfails++))
      fi
    else
      logmsg "Compressed processing file $f was empty"
      rm -f ${f}
   fi
   # If we have failed too many times, break out. We don't want to spend a lot of time
   # cycling through a large queue if the destination is not available
   if [ $_setnfails -ge $PostFailureMax ]; then
     logmsg "Exiting after too many post failures"
     break;
   fi
  done
fi

# Return to base location
cd ${_myloc}
clean_store ${STROOM_LOG_QUEUED} ${FAILED_RETENTION} ${FAILED_MAX}
stroom_rm_lock
exit 0
